"use strict";
var __awaiter = (this && this.__awaiter) || function (thisArg, _arguments, P, generator) {
    function adopt(value) { return value instanceof P ? value : new P(function (resolve) { resolve(value); }); }
    return new (P || (P = Promise))(function (resolve, reject) {
        function fulfilled(value) { try { step(generator.next(value)); } catch (e) { reject(e); } }
        function rejected(value) { try { step(generator["throw"](value)); } catch (e) { reject(e); } }
        function step(result) { result.done ? resolve(result.value) : adopt(result.value).then(fulfilled, rejected); }
        step((generator = generator.apply(thisArg, _arguments || [])).next());
    });
};
var __importDefault = (this && this.__importDefault) || function (mod) {
    return (mod && mod.__esModule) ? mod : { "default": mod };
};
Object.defineProperty(exports, "__esModule", { value: true });
exports.Client = void 0;
const locale_code_1 = __importDefault(require("locale-code"));
const uuid_1 = require("uuid");
const token_1 = require("../websocket/token");
const microphone_1 = require("../microphone");
const websocket_1 = require("../websocket");
const storage_1 = require("../storage");
const types_1 = require("./types");
const state_1 = require("./state");
const segment_1 = require("./segment");
const parsers_1 = require("./parsers");
const deviceIdStorageKey = 'speechly-device-id';
const authTokenKey = 'speechly-auth-token';
const defaultApiUrl = 'wss://api.speechly.com/ws/v1';
const defaultLoginUrl = 'https://api.speechly.com/login';
const defaultLanguage = 'en-US';
/**
 * A client for Speechly Spoken Language Understanding (SLU) API. The client handles initializing the microphone
 * and websocket connection to Speechly API, passing control events and audio stream to the API, reading the responses
 * and dispatching them, as well as providing a high-level API for interacting with so-called speech segments.
 * @public
 */
class Client {
    constructor(options) {
        var _a, _b, _c, _d, _e, _f, _g, _h, _j, _k, _l, _m;
        this.activeContexts = new Map();
        this.reconnectAttemptCount = 5;
        this.reconnectMinDelay = 1000;
        this.contextStopDelay = 250;
        this.state = types_1.ClientState.Disconnected;
        this.stateChangeCb = () => { };
        this.segmentChangeCb = () => { };
        this.tentativeTranscriptCb = () => { };
        this.tentativeEntitiesCb = () => { };
        this.tentativeIntentCb = () => { };
        this.transcriptCb = () => { };
        this.entityCb = () => { };
        this.intentCb = () => { };
        this.handleWebsocketResponse = (response) => {
            var _a;
            if (this.debug) {
                console.log('[SpeechlyClient]', 'Received response', response);
            }
            const { audio_context, segment_id, type } = response;
            let { data } = response;
            const context = this.activeContexts.get(audio_context);
            if (context === undefined) {
                console.warn('[SpeechlyClient]', 'Received response for non-existent context', audio_context);
                return;
            }
            let segmentState = (_a = context.get(segment_id)) !== null && _a !== void 0 ? _a : new segment_1.SegmentState(audio_context, segment_id);
            switch (type) {
                case websocket_1.WebsocketResponseType.TentativeTranscript:
                    data = data;
                    const words = (0, parsers_1.parseTentativeTranscript)(data);
                    this.tentativeTranscriptCb(audio_context, segment_id, words, data.transcript);
                    segmentState = segmentState.updateTranscript(words);
                    break;
                case websocket_1.WebsocketResponseType.Transcript:
                    data = data;
                    const word = (0, parsers_1.parseTranscript)(data);
                    this.transcriptCb(audio_context, segment_id, word);
                    segmentState = segmentState.updateTranscript([word]);
                    break;
                case websocket_1.WebsocketResponseType.TentativeEntities:
                    data = data;
                    const entities = (0, parsers_1.parseTentativeEntities)(data);
                    this.tentativeEntitiesCb(audio_context, segment_id, entities);
                    segmentState = segmentState.updateEntities(entities);
                    break;
                case websocket_1.WebsocketResponseType.Entity:
                    data = data;
                    const entity = (0, parsers_1.parseEntity)(data);
                    this.entityCb(audio_context, segment_id, entity);
                    segmentState = segmentState.updateEntities([entity]);
                    break;
                case websocket_1.WebsocketResponseType.TentativeIntent:
                    data = data;
                    const tentativeIntent = (0, parsers_1.parseIntent)(data, false);
                    this.tentativeIntentCb(audio_context, segment_id, tentativeIntent);
                    segmentState = segmentState.updateIntent(tentativeIntent);
                    break;
                case websocket_1.WebsocketResponseType.Intent:
                    data = data;
                    const intent = (0, parsers_1.parseIntent)(data, true);
                    this.intentCb(audio_context, segment_id, intent);
                    segmentState = segmentState.updateIntent(intent);
                    break;
                case websocket_1.WebsocketResponseType.SegmentEnd:
                    segmentState = segmentState.finalize();
                    break;
                default:
                // TODO: handle unexpected response types.
            }
            // Update the segment in current context.
            context.set(segment_id, segmentState);
            // Update current contexts.
            this.activeContexts.set(audio_context, context);
            // Log segment to console
            if (this.logSegments) {
                console.info(segmentState.toString());
            }
            // Fire segment change event.
            this.segmentChangeCb(segmentState.toSegment());
        };
        this.handleWebsocketClosure = (err) => {
            if (this.debug) {
                console.error('[SpeechlyClient]', 'Server connection closed', err);
            }
            // If for some reason deviceId is missing, there's nothing else we can do but fail completely.
            if (this.deviceId === undefined) {
                this.setState(types_1.ClientState.Failed);
                return;
            }
            // Make sure we don't have concurrent reconnection procedures or attempt to reconnect from a failed state.
            if (this.state === types_1.ClientState.Connecting || this.state === types_1.ClientState.Failed) {
                return;
            }
            this.setState(types_1.ClientState.Connecting);
        };
        this.sampleRate = (_a = options.sampleRate) !== null && _a !== void 0 ? _a : microphone_1.DefaultSampleRate;
        try {
            const constraints = window.navigator.mediaDevices.getSupportedConstraints();
            this.nativeResamplingSupported = constraints.sampleRate === true;
            if (options.autoGainControl != null && options.autoGainControl) {
                // @ts-ignore
                this.autoGainControl = constraints.autoGainControl === true;
            }
            else {
                this.autoGainControl = false;
            }
        }
        catch (_o) {
            this.nativeResamplingSupported = false;
            this.autoGainControl = false;
        }
        const language = (_b = options.language) !== null && _b !== void 0 ? _b : defaultLanguage;
        if (!(locale_code_1.default.validate(language) || (locale_code_1.default.validateLanguageCode(`${language.substring(0, 2)}-XX`) && /^..-\d\d\d$/.test(language)))) {
            throw Error(`[SpeechlyClient] Invalid language "${language}"`);
        }
        this.debug = (_c = options.debug) !== null && _c !== void 0 ? _c : false;
        this.logSegments = (_d = options.logSegments) !== null && _d !== void 0 ? _d : false;
        this.loginUrl = (_e = options.loginUrl) !== null && _e !== void 0 ? _e : defaultLoginUrl;
        this.appId = (_f = options.appId) !== null && _f !== void 0 ? _f : undefined;
        this.projectId = (_g = options.projectId) !== null && _g !== void 0 ? _g : undefined;
        const apiUrl = generateWsUrl((_h = options.apiUrl) !== null && _h !== void 0 ? _h : defaultApiUrl, language, (_j = options.sampleRate) !== null && _j !== void 0 ? _j : microphone_1.DefaultSampleRate);
        this.apiClient = (_k = options.apiClient) !== null && _k !== void 0 ? _k : new websocket_1.WebWorkerController();
        if (this.appId !== undefined && this.projectId !== undefined) {
            throw Error('[SpeechlyClient] You cannot use both appId and projectId at the same time');
        }
        this.storage = (_l = options.storage) !== null && _l !== void 0 ? _l : new storage_1.LocalStorage();
        this.deviceId = this.storage.getOrSet(deviceIdStorageKey, uuid_1.v4);
        const storedToken = this.storage.get(authTokenKey);
        // 2. Fetch auth token. It doesn't matter if it's not present.
        this.initializeApiClientPromise = new Promise(resolve => {
            this.resolveInitialization = resolve;
        });
        if (storedToken == null || !(0, token_1.validateToken)(storedToken, this.projectId, this.appId, this.deviceId)) {
            (0, token_1.fetchToken)(this.loginUrl, this.projectId, this.appId, this.deviceId)
                .then(token => {
                this.authToken = token;
                // Cache the auth token in local storage for future use.
                this.storage.set(authTokenKey, this.authToken);
                this.connect(apiUrl);
            })
                .catch(err => {
                this.setState(types_1.ClientState.Failed);
                throw err;
            });
        }
        else {
            this.authToken = storedToken;
            this.connect(apiUrl);
        }
        if (window.AudioContext !== undefined) {
            this.isWebkit = false;
        }
        else if (window.webkitAudioContext !== undefined) {
            this.isWebkit = true;
        }
        else {
            throw microphone_1.ErrDeviceNotSupported;
        }
        this.microphone = (_m = options.microphone) !== null && _m !== void 0 ? _m : new microphone_1.BrowserMicrophone(this.isWebkit, this.sampleRate, this.apiClient, this.debug);
        this.apiClient.onResponse(this.handleWebsocketResponse);
        this.apiClient.onClose(this.handleWebsocketClosure);
        window.SpeechlyClient = this;
    }
    /**
     * Esteblish websocket connection
     */
    connect(apiUrl) {
        if (this.authToken != null) {
            this.apiClient.initialize(apiUrl, this.authToken, this.sampleRate, this.debug).then(() => {
                if (this.resolveInitialization != null) {
                    this.resolveInitialization();
                }
            }).catch(err => {
                throw err;
            });
        }
    }
    /**
     * Initializes the client, by initializing the microphone and establishing connection to the API.
     *
     * This function HAS to be invoked by a user by e.g. binding it to a button press,
     * or some other user-performed action.
     *
     * If this function is invoked without a user interaction,
     * the microphone functionality will not work due to security restrictions by the browser.
     */
    initialize() {
        return __awaiter(this, void 0, void 0, function* () {
            yield this.initializeApiClientPromise;
            if (this.state !== types_1.ClientState.Disconnected) {
                throw Error('Cannot initialize client - client is not in Disconnected state');
            }
            this.setState(types_1.ClientState.Connecting);
            try {
                // 1. Initialise the storage and fetch deviceId (or generate new one and store it).
                // await this.storage.initialize()
                // this.deviceId = await this.storage.getOrSet(deviceIdStorageKey, uuidv4)
                // 2. Initialise the microphone stack.
                if (this.isWebkit) {
                    if (window.webkitAudioContext !== undefined) {
                        // eslint-disable-next-line new-cap
                        this.audioContext = new window.webkitAudioContext();
                    }
                }
                else {
                    const opts = {};
                    if (this.nativeResamplingSupported) {
                        opts.sampleRate = this.sampleRate;
                    }
                    this.audioContext = new window.AudioContext(opts);
                }
                const mediaStreamConstraints = {
                    video: false,
                };
                if (this.nativeResamplingSupported || this.autoGainControl) {
                    mediaStreamConstraints.audio = {
                        sampleRate: this.sampleRate,
                        // @ts-ignore
                        autoGainControl: this.autoGainControl,
                    };
                }
                else {
                    mediaStreamConstraints.audio = true;
                }
                if (this.audioContext != null) {
                    // Start audio context if we are dealing with a WebKit browser.
                    //
                    // WebKit browsers (e.g. Safari) require to resume the context first,
                    // before obtaining user media by calling `mediaDevices.getUserMedia`.
                    //
                    // If done in a different order, the audio context will resume successfully,
                    // but will emit empty audio buffers.
                    if (this.isWebkit) {
                        yield this.audioContext.resume();
                    }
                    // 3. Initialise websocket.
                    yield this.apiClient.setSourceSampleRate(this.audioContext.sampleRate);
                    this.initializeMicrophonePromise = this.microphone.initialize(this.audioContext, mediaStreamConstraints);
                    yield this.initializeMicrophonePromise;
                }
                else {
                    throw microphone_1.ErrDeviceNotSupported;
                }
            }
            catch (err) {
                switch (err) {
                    case microphone_1.ErrDeviceNotSupported:
                        this.setState(types_1.ClientState.NoBrowserSupport);
                        break;
                    case microphone_1.ErrNoAudioConsent:
                        this.setState(types_1.ClientState.NoAudioConsent);
                        break;
                    default:
                        this.setState(types_1.ClientState.Failed);
                }
                throw err;
            }
            this.setState(types_1.ClientState.Connected);
        });
    }
    /**
     * Closes the client by closing the API connection and disabling the microphone.
     */
    close() {
        return __awaiter(this, void 0, void 0, function* () {
            const errs = [];
            try {
                yield this.microphone.close();
            }
            catch (err) {
                // @ts-ignore
                errs.push(err.message);
            }
            try {
                yield this.apiClient.close();
            }
            catch (err) {
                // @ts-ignore
                errs.push(err.message);
            }
            this.activeContexts.clear();
            this.setState(types_1.ClientState.Disconnected);
            if (errs.length > 0) {
                throw Error(errs.join(','));
            }
        });
    }
    /**
     * Stops current context and immediately starts a new SLU context
     * by sending a start context event to the API and unmuting the microphone.
     * @param appId - unique identifier of an app in the dashboard.
     */
    switchContext(appId) {
        return __awaiter(this, void 0, void 0, function* () {
            if (this.state === types_1.ClientState.Recording) {
                this.resolveStopContext = undefined;
                const contextId = yield this.apiClient.switchContext(appId);
                this.activeContexts.set(contextId, new Map());
            }
        });
    }
    /**
     * Starts a new SLU context by sending a start context event to the API and unmuting the microphone.
     * @param cb - the callback which is invoked when the context start was acknowledged by the API.
     */
    startContext(appId) {
        return __awaiter(this, void 0, void 0, function* () {
            if (this.resolveStopContext != null) {
                this.resolveStopContext();
                yield this.stoppedContextIdPromise;
            }
            if (this.state === types_1.ClientState.Disconnected || this.state === types_1.ClientState.Connecting) {
                throw Error('Cannot start context - client is not connected');
            }
            this.setState(types_1.ClientState.Starting);
            const contextId = yield this._startContext(appId);
            return contextId;
        });
    }
    _startContext(appId) {
        return __awaiter(this, void 0, void 0, function* () {
            let contextId;
            try {
                if (this.projectId != null) {
                    contextId = yield this.apiClient.startContext(appId);
                }
                else {
                    if (appId != null && this.appId !== appId) {
                        throw microphone_1.ErrAppIdChangeWithoutProjectLogin;
                    }
                    contextId = yield this.apiClient.startContext();
                }
            }
            catch (err) {
                switch (err) {
                    case microphone_1.ErrAppIdChangeWithoutProjectLogin:
                        this.setState(types_1.ClientState.Failed);
                        break;
                    default:
                        this.setState(types_1.ClientState.Connected);
                }
                throw err;
            }
            this.setState(types_1.ClientState.Recording);
            this.microphone.unmute();
            this.activeContexts.set(contextId, new Map());
            return contextId;
        });
    }
    /**
     * Stops current SLU context by sending a stop context event to the API and muting the microphone
     * delayed by contextStopDelay = 250 ms
     */
    stopContext() {
        return __awaiter(this, void 0, void 0, function* () {
            if (this.state !== types_1.ClientState.Recording && this.state !== types_1.ClientState.Starting) {
                throw Error('Cannot stop context - client is not recording');
            }
            this.setState(types_1.ClientState.Stopping);
            this.stoppedContextIdPromise = new Promise(resolve => {
                Promise.race([
                    new Promise(resolve => setTimeout(resolve, this.contextStopDelay)),
                    new Promise(resolve => {
                        this.resolveStopContext = resolve;
                    }),
                ])
                    .then(() => {
                    this._stopContext()
                        .then(id => {
                        resolve(id);
                    })
                        .catch(err => {
                        throw err;
                    });
                })
                    .catch(err => {
                    throw err;
                });
            });
            const contextId = yield this.stoppedContextIdPromise;
            this.setState(types_1.ClientState.Connected);
            this.activeContexts.delete(contextId);
            return contextId;
        });
    }
    _stopContext() {
        return __awaiter(this, void 0, void 0, function* () {
            this.microphone.mute();
            let contextId;
            try {
                contextId = yield this.apiClient.stopContext();
            }
            catch (err) {
                this.setState(types_1.ClientState.Failed);
                throw err;
            }
            return contextId;
        });
    }
    /**
     * Adds a listener for client state change events.
     * @param cb - the callback to invoke on state change events.
     */
    onStateChange(cb) {
        this.stateChangeCb = cb;
    }
    /**
     * Adds a listener for current segment change events.
     * @param cb - the callback to invoke on segment change events.
     */
    onSegmentChange(cb) {
        this.segmentChangeCb = cb;
    }
    /**
     * Adds a listener for tentative transcript responses from the API.
     * @param cb - the callback to invoke on a tentative transcript response.
     */
    onTentativeTranscript(cb) {
        this.tentativeTranscriptCb = cb;
    }
    /**
     * Adds a listener for transcript responses from the API.
     * @param cb - the callback to invoke on a transcript response.
     */
    onTranscript(cb) {
        this.transcriptCb = cb;
    }
    /**
     * Adds a listener for tentative entities responses from the API.
     * @param cb - the callback to invoke on a tentative entities response.
     */
    onTentativeEntities(cb) {
        this.tentativeEntitiesCb = cb;
    }
    /**
     * Adds a listener for entity responses from the API.
     * @param cb - the callback to invoke on an entity response.
     */
    onEntity(cb) {
        this.entityCb = cb;
    }
    /**
     * Adds a listener for tentative intent responses from the API.
     * @param cb - the callback to invoke on a tentative intent response.
     */
    onTentativeIntent(cb) {
        this.tentativeIntentCb = cb;
    }
    /**
     * Adds a listener for intent responses from the API.
     * @param cb - the callback to invoke on an intent response.
     */
    onIntent(cb) {
        this.intentCb = cb;
    }
    setState(newState) {
        if (this.state === newState) {
            return;
        }
        if (this.debug) {
            console.log('[SpeechlyClient]', 'State transition', (0, state_1.stateToString)(this.state), (0, state_1.stateToString)(newState));
        }
        this.state = newState;
        this.stateChangeCb(newState);
    }
    /**
     * print statistics to console
     */
    printStats() {
        this.microphone.printStats();
    }
}
exports.Client = Client;
function generateWsUrl(baseUrl, languageCode, sampleRate) {
    const params = new URLSearchParams();
    params.append('languageCode', languageCode);
    params.append('sampleRate', sampleRate.toString());
    return `${baseUrl}?${params.toString()}`;
}
//# sourceMappingURL=client.js.map